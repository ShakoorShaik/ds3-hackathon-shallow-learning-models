{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Training Data: 100%|██████████| 4000/4000 [00:02<00:00, 1461.99it/s]\n",
      "Loading Validation Data: 100%|██████████| 1000/1000 [00:00<00:00, 1558.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 522ms/step - accuracy: 0.7398 - loss: 0.7397 - val_accuracy: 0.8580 - val_loss: 0.2664\n",
      "Epoch 2/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 500ms/step - accuracy: 0.8759 - loss: 0.2536 - val_accuracy: 0.8590 - val_loss: 0.2847\n",
      "Epoch 3/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 477ms/step - accuracy: 0.8584 - loss: 0.3046 - val_accuracy: 0.9050 - val_loss: 0.2024\n",
      "Epoch 4/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 479ms/step - accuracy: 0.9157 - loss: 0.2001 - val_accuracy: 0.8960 - val_loss: 0.2361\n",
      "Epoch 5/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 496ms/step - accuracy: 0.8974 - loss: 0.2185 - val_accuracy: 0.9090 - val_loss: 0.2090\n",
      "Epoch 6/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 494ms/step - accuracy: 0.9110 - loss: 0.1995 - val_accuracy: 0.9080 - val_loss: 0.2013\n",
      "Epoch 7/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 495ms/step - accuracy: 0.9200 - loss: 0.1892 - val_accuracy: 0.8860 - val_loss: 0.2358\n",
      "Epoch 8/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 498ms/step - accuracy: 0.9224 - loss: 0.1781 - val_accuracy: 0.9090 - val_loss: 0.1855\n",
      "Epoch 9/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 501ms/step - accuracy: 0.9279 - loss: 0.1635 - val_accuracy: 0.8510 - val_loss: 0.3245\n",
      "Epoch 10/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 506ms/step - accuracy: 0.9101 - loss: 0.1926 - val_accuracy: 0.8550 - val_loss: 0.2754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ORB Features: 100%|██████████| 5000/5000 [00:09<00:00, 547.27it/s]\n",
      "Classifying Test Images: 100%|██████████| 1801/1801 [03:06<00:00,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction complete. Results saved to submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset CSVs\n",
    "df_train = pd.read_csv(\"fungi_train.csv\")\n",
    "df_test = pd.read_csv(\"fungi_test.csv\")\n",
    "\n",
    "df_train[\"Path\"] = df_train[\"Path\"].apply(os.path.abspath)\n",
    "df_test[\"Path\"] = df_test[\"Path\"].apply(os.path.abspath)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df_train[\"ClassId\"] = encoder.fit_transform(df_train[\"ClassId\"])\n",
    "num_classes = len(df_train[\"ClassId\"].unique())\n",
    "\n",
    "# Image properties\n",
    "img_height, img_width = 180, 180\n",
    "batch_size = 32\n",
    "\n",
    "# Load and preprocess images efficiently\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "    return cv2.resize(img, (img_height, img_width))\n",
    "\n",
    "# Ensure 80/20 split for each class\n",
    "train_images, val_images = [], []\n",
    "train_labels, val_labels = [], []\n",
    "\n",
    "for class_id in df_train[\"ClassId\"].unique():\n",
    "    class_subset = df_train[df_train[\"ClassId\"] == class_id]\n",
    "    \n",
    "    # Ensure shuffling before splitting\n",
    "    class_subset = class_subset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    split_idx = int(0.8 * len(class_subset))  # 80% training, 20% validation\n",
    "    \n",
    "    train_images.extend(class_subset.iloc[:split_idx][\"Path\"].tolist())\n",
    "    val_images.extend(class_subset.iloc[split_idx:][\"Path\"].tolist())\n",
    "\n",
    "    train_labels.extend(class_subset.iloc[:split_idx][\"ClassId\"].tolist())\n",
    "    val_labels.extend(class_subset.iloc[split_idx:][\"ClassId\"].tolist())\n",
    "\n",
    "# Convert lists into arrays\n",
    "X_train = np.array([load_and_preprocess_image(path) for path in tqdm(train_images, desc=\"Loading Training Data\")])\n",
    "X_val = np.array([load_and_preprocess_image(path) for path in tqdm(val_images, desc=\"Loading Validation Data\")])\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "# Define ResNet50 model\n",
    "resnet_model = keras.Sequential()\n",
    "pretrained_model = tf.keras.applications.ResNet50(include_top=False,\n",
    "                                                  input_shape=(img_height, img_width, 3),\n",
    "                                                  pooling='avg',\n",
    "                                                  weights='imagenet')\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "resnet_model.add(pretrained_model)\n",
    "resnet_model.add(keras.layers.Flatten())\n",
    "resnet_model.add(keras.layers.Dense(512, activation='relu'))\n",
    "resnet_model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "resnet_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = resnet_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# ORB Feature Extraction for Backup Matching\n",
    "orb = cv2.ORB_create(nfeatures=30)\n",
    "class_descriptors = {i: [] for i in df_train[\"ClassId\"].unique()}\n",
    "\n",
    "def extract_orb_features(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "    return orb.detectAndCompute(img, None)[1]  # Return only descriptors\n",
    "\n",
    "for row in tqdm(df_train.itertuples(), total=len(df_train), desc=\"Extracting ORB Features\"):\n",
    "    descriptors = extract_orb_features(row.Path)\n",
    "    if descriptors is not None:\n",
    "        class_descriptors[row.ClassId].append(descriptors)\n",
    "\n",
    "def match_features(test_desc, train_desc_list):\n",
    "    if test_desc is None:\n",
    "        return 0\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "    best_matches = 0\n",
    "    \n",
    "    for train_desc in train_desc_list:\n",
    "        if train_desc is None:\n",
    "            continue\n",
    "        matches = matcher.knnMatch(test_desc, train_desc, k=2)\n",
    "        good_matches = [m for match in matches if len(match) == 2 for m, n in [match] if m.distance < 0.7 * n.distance]\n",
    "        best_matches = max(best_matches, len(good_matches))\n",
    "    \n",
    "    return best_matches\n",
    "\n",
    "# Test Image Classification\n",
    "predictions = []\n",
    "for row in tqdm(df_test.itertuples(), total=len(df_test), desc=\"Classifying Test Images\"):\n",
    "    img = np.expand_dims(load_and_preprocess_image(row.Path), axis=0)\n",
    "    cnn_pred = np.argmax(resnet_model.predict(img, verbose=0))\n",
    "    \n",
    "    test_desc = extract_orb_features(row.Path)\n",
    "    match_scores = {class_id: match_features(test_desc, class_descriptors[class_id]) for class_id in class_descriptors}\n",
    "    best_match = max(match_scores, key=match_scores.get)\n",
    "    \n",
    "    final_pred = cnn_pred if match_scores[best_match] < 10 else best_match\n",
    "    predictions.append(final_pred)\n",
    "\n",
    "# Save results\n",
    "df_submission = pd.DataFrame({\"id\": df_test[\"id\"], \"output\": predictions})\n",
    "df_submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Prediction complete. Results saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Training Data: 100%|██████████| 5000/5000 [00:04<00:00, 1057.92it/s]\n",
      "c:\\Documents\\DATATHON\\fungi-classification-shallow-learning\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 2s/step - accuracy: 0.1974 - loss: 1.6094 - val_accuracy: 0.2088 - val_loss: 1.6605\n",
      "Epoch 2/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 2s/step - accuracy: 0.2022 - loss: 1.6094 - val_accuracy: 0.2088 - val_loss: 1.6292\n",
      "Epoch 3/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 2s/step - accuracy: 0.2024 - loss: 1.6094 - val_accuracy: 0.2088 - val_loss: 1.6200\n",
      "Epoch 4/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 2s/step - accuracy: 0.2098 - loss: 1.6094 - val_accuracy: 0.2088 - val_loss: 1.6158\n",
      "Epoch 5/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.2023 - loss: 1.6094 - val_accuracy: 0.0000e+00 - val_loss: 1.6159\n",
      "Epoch 6/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.2066 - loss: 1.6094 - val_accuracy: 0.1980 - val_loss: 1.6153\n",
      "Epoch 7/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.2136 - loss: 1.6093 - val_accuracy: 0.1980 - val_loss: 1.6340\n",
      "Epoch 8/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.2103 - loss: 1.6094 - val_accuracy: 0.1980 - val_loss: 1.6490\n",
      "Epoch 9/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.2134 - loss: 1.6094 - val_accuracy: 0.1980 - val_loss: 1.6424\n",
      "Epoch 10/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.2240 - loss: 1.6092 - val_accuracy: 0.1980 - val_loss: 1.6298\n",
      "Epoch 1/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.1972 - loss: 1.6095 - val_accuracy: 0.1158 - val_loss: 1.6237\n",
      "Epoch 2/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.1872 - loss: 1.6097 - val_accuracy: 0.0000e+00 - val_loss: 1.6340\n",
      "Epoch 3/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.2003 - loss: 1.6094 - val_accuracy: 0.1992 - val_loss: 1.6326\n",
      "Epoch 4/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.1868 - loss: 1.6095 - val_accuracy: 0.1992 - val_loss: 1.6329\n",
      "Epoch 5/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.1985 - loss: 1.6093 - val_accuracy: 0.3107 - val_loss: 1.6326\n",
      "Epoch 6/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.2048 - loss: 1.6094 - val_accuracy: 0.2058 - val_loss: 1.6360\n",
      "Epoch 1/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.1950 - loss: 1.6095 - val_accuracy: 0.3679 - val_loss: 1.6224\n",
      "Epoch 2/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.1874 - loss: 1.6095 - val_accuracy: 0.0306 - val_loss: 1.6163\n",
      "Epoch 3/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.1924 - loss: 1.6094 - val_accuracy: 0.2029 - val_loss: 1.6312\n",
      "Epoch 4/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.1952 - loss: 1.6094 - val_accuracy: 0.2029 - val_loss: 1.6386\n",
      "Epoch 5/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.1938 - loss: 1.6095 - val_accuracy: 0.2029 - val_loss: 1.6314\n",
      "Epoch 6/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.2045 - loss: 1.6094 - val_accuracy: 0.2029 - val_loss: 1.6300\n",
      "Epoch 7/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.2011 - loss: 1.6095 - val_accuracy: 0.2029 - val_loss: 1.6498\n",
      "Average K-Fold Accuracy: 0.2958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ORB Features: 100%|██████████| 5000/5000 [00:16<00:00, 294.75it/s]\n",
      "Classifying Test Images: 100%|██████████| 1801/1801 [11:02<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction complete. Results saved to submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras._tf_keras.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load dataset CSVs\n",
    "df_train = pd.read_csv(\"fungi_train.csv\")\n",
    "df_test = pd.read_csv(\"fungi_test.csv\")\n",
    "\n",
    "df_train[\"Path\"] = df_train[\"Path\"].apply(os.path.abspath)\n",
    "df_test[\"Path\"] = df_test[\"Path\"].apply(os.path.abspath)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "df_train[\"ClassId\"] = encoder.fit_transform(df_train[\"ClassId\"])\n",
    "num_classes = len(df_train[\"ClassId\"].unique())\n",
    "\n",
    "# Image properties\n",
    "img_height, img_width = 180, 180\n",
    "batch_size = 32\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = keras.preprocessing.image.load_img(image_path, target_size=(img_height, img_width))\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    return img_array\n",
    "\n",
    "# Advanced Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3]\n",
    ")\n",
    "\n",
    "def contrast_stretch(img):\n",
    "    \"\"\"Apply contrast stretching using OpenCV.\"\"\"\n",
    "    min_val, max_val = np.min(img), np.max(img)\n",
    "    img_stretched = (img - min_val) / (max_val - min_val + 1e-5)  # Normalize\n",
    "    return np.clip(img_stretched, 0, 1)\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = keras.preprocessing.image.load_img(image_path, target_size=(img_height, img_width))\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = contrast_stretch(img_array)  # Apply contrast stretching\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    return img_array\n",
    "\n",
    "# Load all images and labels\n",
    "train_images = df_train[\"Path\"].tolist()\n",
    "train_labels = df_train[\"ClassId\"].tolist()\n",
    "X_train = np.array([load_and_preprocess_image(path) for path in tqdm(train_images, desc=\"Loading Training Data\")])\n",
    "y_train = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "\n",
    "# Define VGG16 model with improvements\n",
    "base_model = tf.keras.applications.VGG16(include_top=False, input_shape=(img_height, img_width, 3), pooling='avg', weights='imagenet')\n",
    "for layer in base_model.layers[:-6]:  # Unfreeze last 6 layers for fine-tuning\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    history = model.fit(datagen.flow(X_tr, y_tr, batch_size=batch_size), validation_data=(X_val, y_val), epochs=10, verbose=1, callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])\n",
    "    \n",
    "    fold_acc = max(history.history['val_accuracy'])\n",
    "    fold_accuracies.append(fold_acc)\n",
    "\n",
    "print(f'Average K-Fold Accuracy: {np.mean(fold_accuracies):.4f}')\n",
    "\n",
    "# ORB Feature Extraction for Backup Matching\n",
    "orb = cv2.ORB_create(nfeatures=150)\n",
    "class_descriptors = {i: [] for i in df_train[\"ClassId\"].unique()}\n",
    "\n",
    "def extract_orb_features(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "    keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "    return descriptors\n",
    "\n",
    "for row in tqdm(df_train.itertuples(), total=len(df_train), desc=\"Extracting ORB Features\"):\n",
    "    descriptors = extract_orb_features(row.Path)\n",
    "    if descriptors is not None:\n",
    "        class_descriptors[row.ClassId].append(descriptors)\n",
    "\n",
    "# Test Image Classification\n",
    "predictions = []\n",
    "for row in tqdm(df_test.itertuples(), total=len(df_test), desc=\"Classifying Test Images\"):\n",
    "    img = np.expand_dims(load_and_preprocess_image(row.Path), axis=0)\n",
    "    cnn_pred = np.argmax(model.predict(img, verbose=0))\n",
    "    cnn_probs = model.predict(img, verbose=0)[0]\n",
    "    \n",
    "    test_desc = extract_orb_features(row.Path)\n",
    "    match_scores = {class_id: max([len(cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True).match(test_desc, desc)) if desc is not None else 0 for desc in class_descriptors[class_id]]) for class_id in class_descriptors}\n",
    "    best_match = max(match_scores, key=match_scores.get)\n",
    "    \n",
    "    # Adjust predictions to avoid class 4 bias\n",
    "    if cnn_pred == 4 and (cnn_probs[4] < 0.5 or cnn_probs[0] > 0.3 or cnn_probs[1] > 0.3):\n",
    "        final_pred = np.argmax([cnn_probs[0] + 0.1, cnn_probs[1] + 0.1, cnn_probs[2], cnn_probs[3], cnn_probs[4] - 0.1])\n",
    "    else:\n",
    "        final_pred = cnn_pred if match_scores[best_match] < 20 else best_match\n",
    "    \n",
    "    predictions.append(final_pred)\n",
    "\n",
    "# Save results\n",
    "df_submission = pd.DataFrame({\"id\": df_test[\"id\"], \"output\": predictions})\n",
    "df_submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Prediction complete. Results saved to submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 validated image filenames belonging to 5 classes.\n",
      "Found 1000 validated image filenames belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Documents\\DATATHON\\fungi-classification-shallow-learning\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.4261 - loss: 1.5125 - val_accuracy: 0.0070 - val_loss: 2.1379 - learning_rate: 1.0000e-04\n",
      "Epoch 2/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.6729 - loss: 0.7597 - val_accuracy: 0.0010 - val_loss: 2.2402 - learning_rate: 1.0000e-04\n",
      "Epoch 3/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947ms/step - accuracy: 0.6667 - loss: 0.7190\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.6674 - loss: 0.7182 - val_accuracy: 0.0010 - val_loss: 2.3378 - learning_rate: 1.0000e-04\n",
      "Epoch 4/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.7099 - loss: 0.6117 - val_accuracy: 0.0030 - val_loss: 2.4564 - learning_rate: 5.0000e-05\n",
      "Epoch 5/6\n",
      "\u001b[1m 1/31\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 921ms/step - accuracy: 0.8125 - loss: 0.6446"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Documents\\DATATHON\\fungi-classification-shallow-learning\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 982ms/step - accuracy: 0.8125 - loss: 0.6446 - val_accuracy: 0.0000e+00 - val_loss: 2.4782 - learning_rate: 5.0000e-05\n",
      "Epoch 6/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.7301 - loss: 0.5820 - val_accuracy: 0.0020 - val_loss: 2.5973 - learning_rate: 2.5000e-05\n",
      "Epoch 1/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.7148 - loss: 0.6305 - val_accuracy: 0.0030 - val_loss: 2.7397 - learning_rate: 2.5000e-05\n",
      "Epoch 2/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.7022 - loss: 0.6122 - val_accuracy: 0.0010 - val_loss: 2.9356 - learning_rate: 2.5000e-05\n",
      "Epoch 3/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937ms/step - accuracy: 0.7339 - loss: 0.5971\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.7339 - loss: 0.5965 - val_accuracy: 0.0010 - val_loss: 3.0671 - learning_rate: 2.5000e-05\n",
      "Epoch 4/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.7457 - loss: 0.5482 - val_accuracy: 0.0020 - val_loss: 3.2262 - learning_rate: 1.2500e-05\n",
      "Epoch 5/6\n",
      "\u001b[1m 1/31\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 909ms/step - accuracy: 0.6250 - loss: 0.9062\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 977ms/step - accuracy: 0.6250 - loss: 0.9062 - val_accuracy: 0.0040 - val_loss: 3.2085 - learning_rate: 1.2500e-05\n",
      "Epoch 6/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.7132 - loss: 0.5789 - val_accuracy: 0.0010 - val_loss: 3.3549 - learning_rate: 6.2500e-06\n",
      "Epoch 1/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.7176 - loss: 0.6033 - val_accuracy: 0.0000e+00 - val_loss: 3.4626 - learning_rate: 6.2500e-06\n",
      "Epoch 2/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.7122 - loss: 0.6132 - val_accuracy: 0.0010 - val_loss: 3.5569 - learning_rate: 6.2500e-06\n",
      "Epoch 3/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941ms/step - accuracy: 0.7218 - loss: 0.5386\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.7219 - loss: 0.5387 - val_accuracy: 0.0050 - val_loss: 3.6590 - learning_rate: 6.2500e-06\n",
      "Epoch 4/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.7232 - loss: 0.5740 - val_accuracy: 0.0030 - val_loss: 3.7248 - learning_rate: 3.1250e-06\n",
      "Epoch 5/6\n",
      "\u001b[1m 1/31\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 906ms/step - accuracy: 0.8125 - loss: 0.4788\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 977ms/step - accuracy: 0.8125 - loss: 0.4788 - val_accuracy: 0.0040 - val_loss: 3.7744 - learning_rate: 3.1250e-06\n",
      "Epoch 6/6\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.7374 - loss: 0.5419 - val_accuracy: 0.0050 - val_loss: 3.8331 - learning_rate: 1.5625e-06\n",
      "Average K-Fold Accuracy: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Test Images: 100%|██████████| 1801/1801 [03:16<00:00,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction complete. Results saved to submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras._tf_keras.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.src.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Enable XLA for faster execution\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Use mixed precision for faster training\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Load dataset CSVs\n",
    "df_train = pd.read_csv(\"fungi_train.csv\")\n",
    "df_test = pd.read_csv(\"fungi_test.csv\")\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "df_train[\"ClassId\"] = encoder.fit_transform(df_train[\"ClassId\"])\n",
    "num_classes = len(df_train[\"ClassId\"].unique())\n",
    "\n",
    "# Image properties\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32  # Reduced batch size for efficiency\n",
    "\n",
    "# Image Preprocessing Function\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (img_width, img_height))\n",
    "    return img / 255.0\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "df_train[\"ClassId\"] = df_train[\"ClassId\"].astype(str)\n",
    "\n",
    "# Generators\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    x_col=\"Path\",\n",
    "    y_col=\"ClassId\",\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    x_col=\"Path\",\n",
    "    y_col=\"ClassId\",\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# Define EfficientNet model for improved efficiency\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=(img_height, img_width, 3), pooling='avg', weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Dynamic Learning Rate Adjustment\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, verbose=1)\n",
    "\n",
    "# Train with K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "for train_idx, val_idx in kf.split(df_train):\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=6,\n",
    "        steps_per_epoch=len(train_generator) // 4,  # Reduce steps per epoch\n",
    "        callbacks=[lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "    fold_acc = max(history.history['val_accuracy'])\n",
    "    fold_accuracies.append(fold_acc)\n",
    "\n",
    "print(f'Average K-Fold Accuracy: {np.mean(fold_accuracies):.4f}')\n",
    "\n",
    "# Test Image Classification\n",
    "predictions = []\n",
    "for row in tqdm(df_test.itertuples(), total=len(df_test), desc=\"Classifying Test Images\"):\n",
    "    img = preprocess_image(row.Path)\n",
    "    if img is None:\n",
    "        predictions.append(-1)\n",
    "        continue\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    cnn_pred = np.argmax(model.predict(img, verbose=0)[0])\n",
    "    predictions.append(cnn_pred)\n",
    "\n",
    "# Save results\n",
    "df_submission = pd.DataFrame({\"id\": df_test[\"id\"], \"output\": predictions})\n",
    "df_submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Prediction complete. Results saved to submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
